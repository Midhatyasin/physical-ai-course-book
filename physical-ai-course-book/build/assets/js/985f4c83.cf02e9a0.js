"use strict";(globalThis.webpackChunkphysical_ai_course_book=globalThis.webpackChunkphysical_ai_course_book||[]).push([[137],{7371(n,e,i){i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module4-vla","title":"Chapter 5: Vision-Language-Action (VLA)","description":"\ud83c\udfaf Learning Objectives","source":"@site/docs/module4-vla.md","sourceDirName":".","slug":"/module4-vla","permalink":"/physical-ai-course-book/module4-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/Midhatyasin/physical-ai-course-book/tree/main/docs/module4-vla.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: NVIDIA Isaac Platform","permalink":"/physical-ai-course-book/module3-isaac"},"next":{"title":"Chapter 6: Capstone Project \u2013 Autonomous Humanoid","permalink":"/physical-ai-course-book/capstone"}}');var l=i(4848),s=i(8453);const t={},o="Chapter 5: Vision-Language-Action (VLA)",a={},c=[{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"5.1 What is Vision-Language-Action (VLA)?",id:"51-what-is-vision-language-action-vla",level:2},{value:"5.2 LLMs in Robotics",id:"52-llms-in-robotics",level:2},{value:"\ud83e\udde0 Why LLMs?",id:"-why-llms",level:3},{value:"\ud83e\uddea Example Prompt",id:"-example-prompt",level:3},{value:"5.3 Voice-to-Action Pipeline",id:"53-voice-to-action-pipeline",level:2},{value:"\ud83c\udfa4 Components",id:"-components",level:3},{value:"\ud83e\uddf1 Architecture",id:"-architecture",level:3},{value:"\ud83d\udcc4 Example Code: Whisper Integration",id:"-example-code-whisper-integration",level:3},{value:"5.4 Cognitive Planning with LLMs",id:"54-cognitive-planning-with-llms",level:2},{value:"\ud83e\udde0 Planning Steps",id:"-planning-steps",level:3},{value:"\ud83e\uddea Example LLM Prompt",id:"-example-llm-prompt",level:3},{value:"5.5 Hands-On: VLA Demo",id:"55-hands-on-vla-demo",level:2},{value:"\ud83d\udcc1 Project Structure",id:"-project-structure",level:3},{value:"\ud83d\udcc4 whisper_node.py",id:"-whisper_nodepy",level:3},{value:"\ud83d\udcc4 planner_node.py",id:"-planner_nodepy",level:3},{value:"5.6 Summary",id:"56-summary",level:2},{value:"\ud83e\uddea Quick Quiz",id:"-quick-quiz",level:2},{value:"\ud83d\udcda Key Takeaways",id:"-key-takeaways",level:2}];function d(n){const e={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.header,{children:(0,l.jsx)(e.h1,{id:"chapter-5-vision-language-action-vla",children:"Chapter 5: Vision-Language-Action (VLA)"})}),"\n",(0,l.jsx)(e.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,l.jsx)(e.p,{children:"By the end of this chapter, you will:"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Understand the VLA paradigm in robotics"}),"\n",(0,l.jsx)(e.li,{children:"Integrate LLMs (e.g., GPT, Gemini) with ROS 2"}),"\n",(0,l.jsx)(e.li,{children:"Build voice-to-action pipelines"}),"\n",(0,l.jsx)(e.li,{children:"Implement cognitive planning for task execution"}),"\n"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"51-what-is-vision-language-action-vla",children:"5.1 What is Vision-Language-Action (VLA)?"}),"\n",(0,l.jsxs)(e.p,{children:[(0,l.jsx)(e.strong,{children:"VLA"})," refers to systems where robots:"]}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"See"})," (Vision: Cameras, LiDAR)"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Understand"})," (Language: Speech/NLP)"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Act"})," (Action: Manipulators, Navigation)"]}),"\n"]}),"\n",(0,l.jsxs)(e.blockquote,{children:["\n",(0,l.jsxs)(e.p,{children:["Think of VLA as the ",(0,l.jsx)(e.strong,{children:"conversational brain"})," of a humanoid robot."]}),"\n"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"52-llms-in-robotics",children:"5.2 LLMs in Robotics"}),"\n",(0,l.jsx)(e.h3,{id:"-why-llms",children:"\ud83e\udde0 Why LLMs?"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Generalization"}),": Handle unseen tasks via prompting"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Planning"}),": Break down complex goals into steps"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Interaction"}),": Enable natural dialogue with users"]}),"\n"]}),"\n",(0,l.jsx)(e.h3,{id:"-example-prompt",children:"\ud83e\uddea Example Prompt"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{children:'User: "Clean the room."\r\nLLM Plan:\r\n1. Navigate to the vacuum cleaner\r\n2. Pick up the vacuum\r\n3. Move to each corner of the room\r\n4. Return vacuum to dock\n'})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"53-voice-to-action-pipeline",children:"5.3 Voice-to-Action Pipeline"}),"\n",(0,l.jsx)(e.h3,{id:"-components",children:"\ud83c\udfa4 Components"}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Speech Recognition"})," (Whisper API)"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Intent Parsing"})," (LLM)"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Action Mapping"})," (ROS 2 Nodes)"]}),"\n"]}),"\n",(0,l.jsx)(e.h3,{id:"-architecture",children:"\ud83e\uddf1 Architecture"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{children:"[Microphone] \r\n    \u2193\r\n[Whisper ASR] \u2192 [LLM Planner] \u2192 [ROS 2 Executor]\r\n    \u2191\r\n[Speaker] \u2190 [Feedback Loop]\n"})}),"\n",(0,l.jsx)(e.h3,{id:"-example-code-whisper-integration",children:"\ud83d\udcc4 Example Code: Whisper Integration"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'import openai\r\n\r\ndef transcribe_audio(audio_file):\r\n    with open(audio_file, "rb") as f:\r\n        transcript = openai.Audio.transcribe("whisper-1", f)\r\n    return transcript[\'text\']\n'})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"54-cognitive-planning-with-llms",children:"5.4 Cognitive Planning with LLMs"}),"\n",(0,l.jsx)(e.h3,{id:"-planning-steps",children:"\ud83e\udde0 Planning Steps"}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Goal Input"}),': "Bring me a bottle of water"']}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Scene Understanding"}),": Detect objects (bottle, table, fridge)"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Path Planning"}),": Navigate to fridge"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Manipulation"}),": Grasp bottle"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Return"}),": Bring to user"]}),"\n"]}),"\n",(0,l.jsx)(e.h3,{id:"-example-llm-prompt",children:"\ud83e\uddea Example LLM Prompt"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{children:"Scene: Fridge at (2,3), Bottle at (2.5,3.2), User at (0,0)\r\nGoal: Bring me a bottle of water\r\nPlan:\r\n1. Navigate to (2.5,3.2)\r\n2. Grasp bottle\r\n3. Navigate to (0,0)\r\n4. Hand over bottle\n"})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"55-hands-on-vla-demo",children:"5.5 Hands-On: VLA Demo"}),"\n",(0,l.jsx)(e.h3,{id:"-project-structure",children:"\ud83d\udcc1 Project Structure"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{children:"/vla_demo\r\n\u251c\u2500\u2500 launch\r\n\u2502   \u2514\u2500\u2500 vla.launch.py\r\n\u251c\u2500\u2500 src\r\n\u2502   \u251c\u2500\u2500 whisper_node.py\r\n\u2502   \u251c\u2500\u2500 planner_node.py\r\n\u2502   \u2514\u2500\u2500 executor_node.py\r\n\u2514\u2500\u2500 config\r\n    \u2514\u2500\u2500 llm_prompt.txt\n"})}),"\n",(0,l.jsx)(e.h3,{id:"-whisper_nodepy",children:"\ud83d\udcc4 whisper_node.py"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass WhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__('whisper_node')\r\n        self.publisher_ = self.create_publisher(String, 'voice_command', 10)\r\n        # Simulate transcription\r\n        msg = String()\r\n        msg.data = \"Bring me a bottle of water\"\r\n        self.publisher_.publish(msg)\r\n        self.get_logger().info('Published voice command')\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = WhisperNode()\r\n    rclpy.spin_once(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,l.jsx)(e.h3,{id:"-planner_nodepy",children:"\ud83d\udcc4 planner_node.py"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass PlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__('planner_node')\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            'voice_command',\r\n            self.plan_callback,\r\n            10\r\n        )\r\n\r\n    def plan_callback(self, msg):\r\n        command = msg.data\r\n        plan = self.generate_plan(command)\r\n        self.get_logger().info(f'Plan: {plan}')\r\n\r\n    def generate_plan(self, command):\r\n        # Simulate LLM call\r\n        return \"1. Navigate to bottle\\n2. Grasp bottle\\n3. Return to user\"\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = PlannerNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"56-summary",children:"5.6 Summary"}),"\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"Component"}),(0,l.jsx)(e.th,{children:"Role"}),(0,l.jsx)(e.th,{children:"Tech Used"})]})}),(0,l.jsxs)(e.tbody,{children:[(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Vision"}),(0,l.jsx)(e.td,{children:"Perceive environment"}),(0,l.jsx)(e.td,{children:"Cameras, Isaac ROS"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Language"}),(0,l.jsx)(e.td,{children:"Understand user intent"}),(0,l.jsx)(e.td,{children:"LLMs (Gemini/GPT)"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:"Action"}),(0,l.jsx)(e.td,{children:"Execute physical tasks"}),(0,l.jsx)(e.td,{children:"ROS 2, Manipulators"})]})]})]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"-quick-quiz",children:"\ud83e\uddea Quick Quiz"}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsx)(e.li,{children:"What does VLA stand for?"}),"\n",(0,l.jsx)(e.li,{children:"How does an LLM help in task planning?"}),"\n",(0,l.jsx)(e.li,{children:"Name two components of a voice-to-action pipeline."}),"\n"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"-key-takeaways",children:"\ud83d\udcda Key Takeaways"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"VLA bridges perception, language, and action"}),"\n",(0,l.jsx)(e.li,{children:"LLMs enable flexible, general-purpose planning"}),"\n",(0,l.jsx)(e.li,{children:"Voice interfaces make robots more accessible"}),"\n",(0,l.jsx)(e.li,{children:"ROS 2 orchestrates the end-to-end pipeline"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,l.jsx)(e,{...n,children:(0,l.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>t,x:()=>o});var r=i(6540);const l={},s=r.createContext(l);function t(n){const e=r.useContext(s);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(l):n.components||l:t(n.components),r.createElement(s.Provider,{value:e},n.children)}}}]);